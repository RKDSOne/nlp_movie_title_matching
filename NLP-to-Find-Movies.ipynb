{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Sklearn NLTK to find closest matching movie title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demonstration, I will use the natural language processing to find a movie title closes to the input text.  \n",
    "I'll using sklearn's Natural Language Tool Kit functions to process text and convert sentences into numeric vectors for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Movie Lens Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is from [movielens](http://files.grouplens.org/datasets/movielens/ml-20m.zip).  The full set of movies.csv is over 27k movie titles and genres.  In this exercise I've taken about 30% of the original data to speed up computing  time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import csv as csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text processing functions\n",
    "\n",
    "import re\n",
    "def filter_non_ascii(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "\n",
    "def filter_carriage_return(text):\n",
    "    return re.sub(r'\\n', ' ', text)\n",
    "\n",
    "def filter_double_quote(text):\n",
    "    return re.sub(r'\\\"', '', text)\n",
    "\n",
    "# Add your domain-specific stopwords\n",
    "def filter_custom_stopwords(text):\n",
    "    custom_stopwords = ['episode', 'season', 'awesomenesstv', 'facebook', 'twitter', \n",
    "                        'instagram', 'tumblr', 'snapchat', 'google',\n",
    "                        'network', 'subscribe', 'download', 'the app', 'follow',\n",
    "                        'ios', 'android', 'http', 'https', 'swag', 'shop', 'youtube support',\n",
    "                        'bitly', 'credits']\n",
    "    for i in custom_stopwords:\n",
    "        text = text.replace(i, ' ')\n",
    "    return text\n",
    "\n",
    "def filter_double_quote(text):\n",
    "    return re.sub(r'\\\"', '', text)\n",
    "\n",
    "def filter_punctuations(text):\n",
    "    return text.translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "\n",
    "# Bulk of the transformation before running tf-idf\n",
    "def parseOutText(text):\n",
    "    if len(text) > 1:\n",
    "        ### remove punctuation\n",
    "        text_no_punctuation = text.translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "        \n",
    "        ### remove custom stopwords - domain specific\n",
    "        text_string = filter_custom_stopwords(text_no_punctuation.lower())\n",
    "        \n",
    "        ### split the text string into individual words, stem each word,\n",
    "        ### and append the stemmed word to words (make sure there's a single\n",
    "        ### space between each stemmed word)\n",
    "        individual_words = word_tokenize(text_string)\n",
    "        \n",
    "        snowball_stemmer = SnowballStemmer(\"english\")\n",
    "        stemmed_text = \"\"\n",
    "        for txt in individual_words:\n",
    "                stemmed_text += snowball_stemmer.stem(txt) + \" \" \n",
    "\n",
    "        words = stemmed_text    \n",
    "\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movielens = pd.read_csv('movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>94494</td>\n",
       "      <td>96 Minutes (2011)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94496</td>\n",
       "      <td>Columbus Circle (2012)</td>\n",
       "      <td>Crime|Mystery|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>94503</td>\n",
       "      <td>Decoy Bride, The (2011)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94531</td>\n",
       "      <td>Headhunter's Sister, The (1997)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94537</td>\n",
       "      <td>Angels Crest (2011)</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   movieId                            title                  genres\n",
       "0    94494               96 Minutes (2011)           Drama|Thriller\n",
       "1    94496           Columbus Circle (2012)  Crime|Mystery|Thriller\n",
       "2    94503          Decoy Bride, The (2011)          Comedy|Romance\n",
       "3    94531  Headhunter's Sister, The (1997)                   Drama\n",
       "4    94537              Angels Crest (2011)                   Drama"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movielens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8278"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movielens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_word_data = movielens['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Gram Document Similarity\n",
    "\n",
    "Here, I didn't remove stop words as the stop words are being used for proper nouns in movie titles such as \"The Shining\" or \"The Martian\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text processing: remove punctuations, stemming and tokenize\n",
    "all_bi_word_data = []\n",
    "\n",
    "# Group 1    \n",
    "# Process description\n",
    "for i, line in enumerate(movie_word_data):\n",
    "    line1 = filter_non_ascii(line)\n",
    "    line2 = parseOutText(line1)\n",
    "    all_bi_word_data.append(line2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8278"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_bi_word_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'96 minut 2011 ',\n",
       " u'columbus circl 2012 ',\n",
       " u'decoy bride the 2011 ',\n",
       " u'headhunt sister the 1997 ',\n",
       " u'angel crest 2011 ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_bi_word_data[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bi-Gram Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In a Bi-Gram algorithm, you can optionally remove stop words, here I didn't.\n",
    "# I'm using bag-of-words and bi-gram by setting ngram_range=(1, 2)\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "\n",
    "bigram_vec = bigram_vectorizer.fit_transform(all_bi_word_data).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From occurrences to frequencies with TF-IDF weighting\n",
    "bi_transformer = TfidfTransformer()\n",
    "\n",
    "tfidf_bi_transform = bi_transformer.fit_transform(bigram_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform a sparse matrix to a dense one so it can be vertically stacked in a later step\n",
    "tfidf_vec = tfidf_bi_transform.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ferguson a',\n",
       " u'ferguson doe',\n",
       " u'ferguson im',\n",
       " u'ferm',\n",
       " u'ferm 2013',\n",
       " u'fernandez',\n",
       " u'fernandez 2012',\n",
       " u'ferngulli',\n",
       " u'ferngulli 2',\n",
       " u'fernsehen']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See sample vector features\n",
    "word_vector = bigram_vectorizer.get_feature_names()\n",
    "word_vector[10000:10010]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 1: Days of Future Past\n",
    "\n",
    "Let's test to see if the algorithm can find the movie title closes to the \"Days of Future Past\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_query = \"Days of Future Past\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_doc_processed = parseOutText(new_query)\n",
    "new_doc_array = [new_doc_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the new query in the pre-trained Bi-Gram Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize new doc\n",
    "new_doc_bigram_vec = bigram_vectorizer.transform(new_doc_array)\n",
    "\n",
    "# From occurrences to frequencies with TF-IDF weighting\n",
    "new_doc_tfidf_bi_transform = bi_transformer.transform(new_doc_bigram_vec)\n",
    "new_doc_matrix = new_doc_tfidf_bi_transform.todense()\n",
    "\n",
    "# Stack new doc as 1st row with the existing Rightsline matrix\n",
    "combo_matrix = np.vstack([new_doc_matrix, tfidf_vec])\n",
    "\n",
    "# Calculate Similarity Score including new_doc\n",
    "sim_score = (combo_matrix * combo_matrix.T).A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the document with the maximum similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_score: 0.76637582957\n",
      "max_index: 4381\n"
     ]
    }
   ],
   "source": [
    "# Find Max Similarity Score and it's index\n",
    "# Note that I don't compare the score with the 0th doc which is itself\n",
    "max_score = np.amax(sim_score[0,1:])\n",
    "max_index = np.argmax(sim_score[0, 1:])\n",
    "    \n",
    "print (\"max_score:\", max_score)\n",
    "print (\"max_index:\", max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4381</th>\n",
       "      <td>111362</td>\n",
       "      <td>X-Men: Days of Future Past (2014)</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                              title                   genres\n",
       "4381   111362  X-Men: Days of Future Past (2014)  Action|Adventure|Sci-Fi"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data from the index of the max similarity score\n",
    "movielens[max_index:max_index+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 2: A Cinderella Story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_query = \"A Cinderella Story\"\n",
    "new_doc_processed = parseOutText(new_query)\n",
    "new_doc_array = [new_doc_processed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the new query in the pre-trained Bi-Gram Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vectorize new doc\n",
    "new_doc_bigram_vec = bigram_vectorizer.transform(new_doc_array)\n",
    "\n",
    "# From occurrences to frequencies with TF-IDF weighting\n",
    "new_doc_tfidf_bi_transform = bi_transformer.transform(new_doc_bigram_vec)\n",
    "new_doc_matrix = new_doc_tfidf_bi_transform.todense()\n",
    "\n",
    "# Stack new doc as 1st row with the existing Rightsline matrix\n",
    "combo_matrix = np.vstack([new_doc_matrix, tfidf_vec])\n",
    "\n",
    "# Calculate Similarity Score including new_doc\n",
    "sim_score = (combo_matrix * combo_matrix.T).A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the document with the maximum similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_score: 0.586858382681\n",
      "max_index: 6102\n"
     ]
    }
   ],
   "source": [
    "# Find Max Similarity Score and it's index\n",
    "# Note that I don't compare the score with the 0th doc which is itself\n",
    "max_score = np.amax(sim_score[0,1:])\n",
    "max_index = np.argmax(sim_score[0, 1:])\n",
    "    \n",
    "print (\"max_score:\", max_score)\n",
    "print (\"max_index:\", max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6102</th>\n",
       "      <td>118496</td>\n",
       "      <td>A Cinderella Story: Once Upon a Song (2011)</td>\n",
       "      <td>Children|Comedy|Romance</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                        title  \\\n",
       "6102   118496  A Cinderella Story: Once Upon a Song (2011)   \n",
       "\n",
       "                       genres  \n",
       "6102  Children|Comedy|Romance  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The data from the index of the max similarity score\n",
    "movielens[max_index:max_index+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This demo shows that using sklearn's NLTK I can find closest movie titles to input text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
